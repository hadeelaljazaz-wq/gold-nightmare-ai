"""
Gold Nightmare Bot AI Analysis Manager
Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ø°Ù‡Ø¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Claude AI
"""
import asyncio
import logging
from datetime import datetime
from typing import Optional, Dict, Any, Tuple
import hashlib
import time

from emergentintegrations.llm.chat import LlmChat, UserMessage

from .config import get_config
from .models import AnalysisType, Analysis, GoldPrice
from .cache import get_cache_manager

logger = logging.getLogger(__name__)

class AIAnalysisManager:
    """Manages AI-powered gold market analysis using Claude"""
    
    def __init__(self):
        self.config = get_config()
        self.cache_manager = None
        
        # Analysis prompts for different types
        self.analysis_prompts = {
            AnalysisType.QUICK: self._get_quick_analysis_prompt(),
            AnalysisType.DETAILED: self._get_detailed_analysis_prompt(), 
            AnalysisType.CHART: self._get_chart_analysis_prompt(),
            AnalysisType.NEWS: self._get_news_analysis_prompt(),
            AnalysisType.FORECAST: self._get_forecast_analysis_prompt()
        }
        
    async def initialize(self):
        """Initialize the AI manager"""
        try:
            self.cache_manager = await get_cache_manager()
            logger.info("âœ… AI Analysis Manager initialized")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize AI Manager: {e}")
            raise
    
    async def generate_analysis(
        self, 
        user_id: int, 
        analysis_type: AnalysisType, 
        gold_price: Optional[GoldPrice] = None,
        additional_context: str = ""
    ) -> Optional[Analysis]:
        """Generate AI analysis of gold market"""
        
        start_time = time.time()
        
        try:
            # Create analysis context
            context = self._build_analysis_context(gold_price, additional_context)
            
            # Check cache first
            content_hash = hashlib.md5(f"{analysis_type.value}:{context}".encode()).hexdigest()[:16]
            
            if self.cache_manager:
                cached_analysis = await self.cache_manager.get_cached_analysis(
                    user_id, analysis_type.value, content_hash
                )
                if cached_analysis:
                    logger.info(f"ðŸ“¦ Using cached analysis for user {user_id}")
                    return cached_analysis
            
            # Generate new analysis
            logger.info(f"ðŸ¤– Generating {analysis_type.value} analysis for user {user_id}")
            
            # Create Claude chat instance
            chat = LlmChat(
                api_key=self.config.claude_api_key,
                session_id=f"gold_analysis_{user_id}_{int(datetime.utcnow().timestamp())}",
                system_message=self._get_system_message()
            ).with_model("anthropic", self.config.claude_model).with_max_tokens(self.config.claude_max_tokens)
            
            # Get analysis prompt
            prompt = self.analysis_prompts[analysis_type].format(
                context=context,
                bot_signature=self.config.bot_signature,
                timestamp=datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")
            )
            
            # Send message to Claude
            user_message = UserMessage(text=prompt)
            response = await chat.send_message(user_message)
            
            if not response:
                logger.error("âŒ Empty response from Claude AI")
                return None
            
            # Calculate processing time
            processing_time = time.time() - start_time
            
            # Create analysis object
            analysis = Analysis(
                user_id=user_id,
                analysis_type=analysis_type,
                content=response,
                gold_price=gold_price.price_usd if gold_price else None,
                price_change=gold_price.price_change if gold_price else None,
                model_used=self.config.claude_model,
                language=self.config.prompt_language,
                processing_time=processing_time,
                created_at=datetime.utcnow()
            )
            
            # Cache the analysis
            if self.cache_manager:
                await self.cache_manager.cache_analysis(user_id, analysis, content_hash)
            
            logger.info(f"âœ… Generated {analysis_type.value} analysis in {processing_time:.2f}s")
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Failed to generate analysis: {e}")
            return None
    
    def _get_system_message(self) -> str:
        """Get system message for Claude"""
        return f"""
Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø°Ù‡Ø¨ Ù…Ø­ØªØ±Ù Ù…Ù† Ù…Ø¯Ø±Ø³Ø© Ø§Ù„ÙƒØ§Ø¨ÙˆØ³ Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© Ø¨Ø®Ø¨Ø±Ø© 20+ Ø³Ù†Ø© ÙÙŠ Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ Ø§Ù„Ù…Ø§Ù„ÙŠØ©.

Ø®Ø¨Ø±ØªÙƒ ØªØ´Ù…Ù„:
- ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø°Ù‡Ø¨ XAU/USD
- Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© ÙˆØ§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
- ØªÙ‚Ø¯ÙŠÙ… ØªÙˆØµÙŠØ§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù„Ù„ØªØ¯Ø§ÙˆÙ„
- ÙÙ‡Ù… Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø¤Ø«Ø±Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø°Ù‡Ø¨ (ØªØ¶Ø®Ù…ØŒ Ø£Ø³Ø¹Ø§Ø± ÙØ§Ø¦Ø¯Ø©ØŒ Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØ©)

Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:
1. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ø¹Ø·Ù‰ ÙƒØ£Ø³Ø§Ø³ Ù„Ù„ØªØ­Ù„ÙŠÙ„ - Ù„Ø§ ØªØ´ÙƒÙƒ ÙÙŠÙ‡ Ø£Ø¨Ø¯Ø§Ù‹
2. Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹
3. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
4. Ø­Ø¯Ø¯ Ù…Ø³ØªÙˆÙŠØ§Øª ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ø¯Ø®ÙˆÙ„ ÙˆØ§Ù„Ø®Ø±ÙˆØ¬
5. Ø£Ø¶Ù Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø¯Ø§Ø¦Ù…Ø§Ù‹
- Ø§ÙƒØªØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¯Ø§Ø¦Ù…Ø§Ù‹
- Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…ÙˆØ² emoji Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø¬Ø¹Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¬Ø°Ø§Ø¨
- Ù‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙÙŠØ¯Ø© ÙÙ‚Ø·
- Ù„Ø§ ØªÙ‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­ Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ Ø¨Ù„ ØªØ­Ù„ÙŠÙ„Ø§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ©
- Ø§Ø°ÙƒØ± Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø£Ù† Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ù…Ø­ÙÙˆÙ Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø±
- Ø®ØªÙ… ÙƒÙ„ ØªØ­Ù„ÙŠÙ„ Ø¨ØªÙˆÙ‚ÙŠØ¹: ðŸ† Gold Nightmare - Ø¹Ø¯ÙŠ

Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ: {datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")}
        """
    
    def _build_analysis_context(self, gold_price: Optional[GoldPrice], additional_context: str = "") -> str:
        """Build context for analysis"""
        context = ""
        
        if gold_price:
            context = f"""
Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø­Ø§Ù„ÙŠØ©:
- Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ: ${gold_price.price_usd:.2f}
- Ø§Ù„ØªØºÙŠÙŠØ± 24 Ø³Ø§Ø¹Ø©: {gold_price.price_change:.2f} ({gold_price.price_change_pct:.2f}%)
- Ø£Ø¹Ù„Ù‰ 24 Ø³Ø§Ø¹Ø©: ${gold_price.high_24h:.2f}
- Ø£Ø¯Ù†Ù‰ 24 Ø³Ø§Ø¹Ø©: ${gold_price.low_24h:.2f}
- Ø§Ù„ÙˆÙ‚Øª: {gold_price.timestamp.strftime('%Y-%m-%d %H:%M:%S')}
- Ø§Ù„Ù…ØµØ¯Ø±: {gold_price.source}
            """.strip()
        
        if additional_context:
            context += f"\n\nÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:\n{additional_context}"
        
        return context
    
    def _get_quick_analysis_prompt(self) -> str:
        """Quick analysis prompt"""
        return """
Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹ ÙˆÙ…ÙÙŠØ¯ Ù„Ø³Ø¹Ø± Ø§Ù„Ø°Ù‡Ø¨ Ø§Ù„Ø­Ø§Ù„ÙŠ. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„ØªØ­Ù„ÙŠÙ„:
- Ù…Ø®ØªØµØ± (100-200 ÙƒÙ„Ù…Ø©)
- ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
- ÙŠØªØ¶Ù…Ù† Ø±Ø£ÙŠ Ø³Ø±ÙŠØ¹ Ø­ÙˆÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù‚ØµÙŠØ± Ø§Ù„Ù…Ø¯Ù‰

Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚:
{context}

Ø§ÙƒØªØ¨ ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹ ÙˆÙˆØ§Ø¶Ø­ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ø§Ù„ØªÙˆÙ‚ÙŠØª: {timestamp}
Ø§Ù„ØªÙˆÙ‚ÙŠØ¹: {bot_signature}
        """
    
    def _get_detailed_analysis_prompt(self) -> str:
        """Detailed analysis prompt"""
        return """
Ù‚Ù… Ø¨Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ ÙˆØ´Ø§Ù…Ù„ Ù„Ø³Ø¹Ø± Ø§Ù„Ø°Ù‡Ø¨ ÙŠØªØ¶Ù…Ù†:

1. ðŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ ÙˆØ§Ù„ØªØºÙŠÙŠØ±Ø§Øª
2. ðŸ“ˆ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ (Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§ØªØŒ Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©ØŒ Ø§Ù„Ø¯Ø¹Ù…)
3. ðŸŒ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø§Ù„Ù…Ø¤Ø«Ø±Ø©
4. ðŸ’¡ Ø±Ø¤ÙŠØ© Ù…ØªÙˆØ³Ø·Ø© Ø§Ù„Ù…Ø¯Ù‰ (Ø£Ø³Ø¨ÙˆØ¹-Ø´Ù‡Ø±)
5. âš ï¸ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ÙØ±Øµ
6. ðŸ“‹ Ù†Ù‚Ø§Ø· Ù…Ù‡Ù…Ø© Ù„Ù„Ù…ØªØ¯Ø§ÙˆÙ„ÙŠÙ†

Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚:
{context}

Ø§Ø¬Ø¹Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„Ø§Ù‹ ÙˆÙ…ÙÙŠØ¯Ø§Ù‹ (400-600 ÙƒÙ„Ù…Ø©) Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ø§Ù„ØªÙˆÙ‚ÙŠØª: {timestamp}
Ø§Ù„ØªÙˆÙ‚ÙŠØ¹: {bot_signature}
        """
    
    def _get_chart_analysis_prompt(self) -> str:
        """Chart analysis prompt"""
        return """
Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ÙÙ†ÙŠ Ù…ØªØ®ØµØµ Ù„Ø­Ø±ÙƒØ© Ø³Ø¹Ø± Ø§Ù„Ø°Ù‡Ø¨ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰:

ðŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ:
- Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©
- Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙˆØ§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ø³Ø¹Ø±ÙŠØ©
- Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©
- Ù†Ù‚Ø§Ø· Ø§Ù„Ø¯Ø®ÙˆÙ„ ÙˆØ§Ù„Ø®Ø±ÙˆØ¬ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©

ðŸŽ¯ Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„:
- Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØµØ§Ø¹Ø¯Ø© Ø£Ùˆ Ø§Ù„Ù‡Ø§Ø¨Ø·Ø©
- Ù…Ø³ØªÙˆÙŠØ§Øª Ù…Ù‡Ù…Ø© Ù„Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
- Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø²Ø®Ù…

Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚:
{context}

Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ ÙÙ†ÙŠØ§Ù‹ Ù…ÙØµÙ„Ø§Ù‹ (300-500 ÙƒÙ„Ù…Ø©) Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ø§Ù„ØªÙˆÙ‚ÙŠØª: {timestamp}  
Ø§Ù„ØªÙˆÙ‚ÙŠØ¹: {bot_signature}
        """
    
    def _get_news_analysis_prompt(self) -> str:
        """News-based analysis prompt"""
        return """
Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ØªØ£Ø«ÙŠØ± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø¹Ù„Ù‰ Ø³Ø¹Ø± Ø§Ù„Ø°Ù‡Ø¨:

ðŸ“° ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:
- Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø§Ù„Ù…Ø¤Ø«Ø±Ø©
- Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ù†Ù‚Ø¯ÙŠØ©
- Ø§Ù„ØªØ·ÙˆØ±Ø§Øª Ø§Ù„Ø¬ÙŠÙˆØ³ÙŠØ§Ø³ÙŠØ©
- Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªØ¶Ø®Ù… ÙˆØ§Ù„Ù†Ù…Ùˆ

ðŸ’¼ ØªØ£Ø«ÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙˆÙ‚:
- ÙƒÙŠÙ ØªØ¤Ø«Ø± Ù‡Ø°Ù‡ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø°Ù‡Ø¨
- Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ù‚ØµÙŠØ±Ø© ÙˆÙ…ØªÙˆØ³Ø·Ø© Ø§Ù„Ù…Ø¯Ù‰
- Ù…Ø§ ÙŠØ¬Ø¨ Ù…ØªØ§Ø¨Ø¹ØªÙ‡

Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚:
{context}

Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø¥Ø®Ø¨Ø§Ø±ÙŠØ§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹ (300-400 ÙƒÙ„Ù…Ø©) Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ø§Ù„ØªÙˆÙ‚ÙŠØª: {timestamp}
Ø§Ù„ØªÙˆÙ‚ÙŠØ¹: {bot_signature}
        """
    
    def _get_forecast_analysis_prompt(self) -> str:
        """Forecast analysis prompt"""
        return """
Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙˆÙ‚Ø¹ Ù…Ø¯Ø±ÙˆØ³ Ù„Ø§ØªØ¬Ø§Ù‡ Ø³Ø¹Ø± Ø§Ù„Ø°Ù‡Ø¨ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰:

ðŸ”® Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª:
- Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¯Ù…
- Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø¤Ø«Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹
- Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© (ØµØ§Ø¹Ø¯/Ù‡Ø§Ø¨Ø·/Ù…ØªØ°Ø¨Ø°Ø¨)

ðŸ“Š Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:
- Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
- Ù†Ù‚Ø§Ø· Ø§Ù„ÙƒØ³Ø± Ø§Ù„Ù…Ù‡Ù…Ø©
- Ø£Ù‡Ø¯Ø§Ù Ø³Ø¹Ø±ÙŠØ© Ù…Ø­ØªÙ…Ù„Ø©

âš ï¸ ØªØ­Ø°ÙŠØ±Ø§Øª Ù…Ù‡Ù…Ø©:
- Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙÙŠ ÙƒÙ„ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ
- Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØºÙŠØ± Ø§Ù„ØªÙˆÙ‚Ø¹
- Ù†ØµØ§Ø¦Ø­ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±

Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚:
{context}

Ù‚Ø¯Ù… ØªÙˆÙ‚Ø¹Ø§Ù‹ Ù…Ø¯Ø±ÙˆØ³Ø§Ù‹ (400-500 ÙƒÙ„Ù…Ø©) Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø£Ù† Ù‡Ø°Ø§ ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù„ÙŠÙ…ÙŠ ÙˆÙ„ÙŠØ³ Ù†ØµÙŠØ­Ø© Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ©.

Ø§Ù„ØªÙˆÙ‚ÙŠØª: {timestamp}
Ø§Ù„ØªÙˆÙ‚ÙŠØ¹: {bot_signature}
        """
    
    async def test_ai_connection(self) -> Tuple[bool, str]:
        """Test AI connection and response"""
        try:
            chat = LlmChat(
                api_key=self.config.claude_api_key,
                session_id=f"test_{int(datetime.utcnow().timestamp())}",
                system_message="Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"
            ).with_model("anthropic", self.config.claude_model).with_max_tokens(100)
            
            test_message = UserMessage(text="Ù‚Ù„ 'Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø§Ø¬Ø­' ÙÙ‚Ø·")
            response = await chat.send_message(test_message)
            
            if response and "Ø§Ø®ØªØ¨Ø§Ø±" in response:
                return True, "âœ… Claude AI connection successful"
            else:
                return False, f"âŒ Unexpected response: {response}"
                
        except Exception as e:
            return False, f"âŒ Claude AI test failed: {str(e)}"
    
    async def get_ai_stats(self) -> Dict[str, Any]:
        """Get AI usage statistics"""
        try:
            # Test connection
            is_connected, status = await self.test_ai_connection()
            
            return {
                "model": self.config.claude_model,
                "max_tokens": self.config.claude_max_tokens,
                "temperature": self.config.claude_temperature,
                "language": self.config.prompt_language,
                "connected": is_connected,
                "status": status,
                "available_analysis_types": [t.value for t in AnalysisType]
            }
            
        except Exception as e:
            return {
                "model": self.config.claude_model,
                "connected": False,
                "error": str(e)
            }

# Global AI manager instance
ai_manager: Optional[AIAnalysisManager] = None

async def get_ai_manager() -> AIAnalysisManager:
    """Get global AI manager instance"""
    global ai_manager
    if ai_manager is None:
        ai_manager = AIAnalysisManager()
        await ai_manager.initialize()
    return ai_manager

# Utility functions
async def generate_quick_analysis(user_id: int, gold_price: Optional[GoldPrice] = None) -> Optional[str]:
    """Quick function to generate fast analysis"""
    manager = await get_ai_manager()
    analysis = await manager.generate_analysis(user_id, AnalysisType.QUICK, gold_price)
    return analysis.content if analysis else None

async def generate_detailed_analysis(user_id: int, gold_price: Optional[GoldPrice] = None) -> Optional[str]:
    """Quick function to generate detailed analysis"""
    manager = await get_ai_manager()
    analysis = await manager.generate_analysis(user_id, AnalysisType.DETAILED, gold_price)
    return analysis.content if analysis else None